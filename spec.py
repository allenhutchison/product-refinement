import argparse
import json
import logging
import os
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any, Union, TypedDict, Callable
import sys
import time
import hashlib
import functools
import pickle
import re

import llm
from dotenv import load_dotenv

# Add colorful output and progress indicators
try:
    from rich.console import Console
    from rich.panel import Panel
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
    from rich.prompt import Prompt
    from rich.table import Table
    from rich.markdown import Markdown
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False
    print("For a better experience, install rich: pip install rich")

# Add validation module
class ValidationError(Exception):
    """Exception raised for validation errors."""
    pass

class Validator:
    """Validation utilities for user input."""
    
    @staticmethod
    def not_empty(value: str, message: str = "Input cannot be empty") -> str:
        """Validate that input is not empty."""
        if not value or value.strip() == "":
            raise ValidationError(message)
        return value.strip()
    
    @staticmethod
    def min_length(value: str, min_len: int, message: Optional[str] = None) -> str:
        """Validate that input has at least min_len characters."""
        if len(value.strip()) < min_len:
            msg = message or f"Input must be at least {min_len} characters"
            raise ValidationError(msg)
        return value.strip()
    
    @staticmethod
    def max_length(value: str, max_len: int, message: Optional[str] = None) -> str:
        """Validate that input has at most max_len characters."""
        if len(value.strip()) > max_len:
            msg = message or f"Input must be at most {max_len} characters"
            raise ValidationError(msg)
        return value.strip()
    
    @staticmethod
    def is_int(value: str, message: str = "Input must be an integer") -> int:
        """Validate that input is an integer."""
        try:
            return int(value.strip())
        except ValueError:
            raise ValidationError(message)
    
    @staticmethod
    def is_float(value: str, message: str = "Input must be a number") -> float:
        """Validate that input is a float."""
        try:
            return float(value.strip())
        except ValueError:
            raise ValidationError(message)
    
    @staticmethod
    def matches_pattern(value: str, pattern: str, message: str = "Input format is invalid") -> str:
        """Validate that input matches a regex pattern."""
        if not re.match(pattern, value.strip()):
            raise ValidationError(message)
        return value.strip()
    
    @staticmethod
    def is_valid_filename(value: str, message: str = "Invalid filename") -> str:
        """Validate that input is a valid filename."""
        # Remove common invalid characters
        value = value.strip()
        if not value or re.search(r'[\\/:*?"<>|]', value):
            raise ValidationError(message)
        return value
    
    @staticmethod
    def is_yes_no(value: str, message: str = "Please enter 'yes' or 'no'") -> bool:
        """Validate that input is a yes/no response and return bool."""
        value = value.strip().lower()
        if value in ('y', 'yes', 'true', '1'):
            return True
        if value in ('n', 'no', 'false', '0'):
            return False
        raise ValidationError(message)
    
    @staticmethod
    def in_choices(value: str, choices: List[str], message: Optional[str] = None) -> str:
        """Validate that input is one of the available choices."""
        value = value.strip().lower()
        choices_lower = [c.lower() for c in choices]
        
        if value not in choices_lower:
            msg = message or f"Input must be one of: {', '.join(choices)}"
            raise ValidationError(msg)
        
        # Return the original case version
        return choices[choices_lower.index(value)]

class Question(TypedDict):
    """Represents a question generated by the AI."""
    section: str
    question: str

class AnsweredQuestion(Question):
    """Represents a question that has been answered by the user."""
    answer: str

class SpecificationVersion(TypedDict):
    """Represents a version of a specification."""
    filename: str
    version: int
    timestamp: str
    formatted_timestamp: str
    product_name: str

class SpecificationData(TypedDict):
    """Represents the data structure of a specification JSON file."""
    version: int
    timestamp: str
    formatted_timestamp: str
    product_name: str
    specification: str

class Config:
    """Application configuration settings."""
    MODEL_NAME: str = "gemini-2.0-flash"  # Default model
    LOG_LEVEL: str = "INFO"  # Default log level
    PROMPT_DIR: str = "prompts"
    SPECS_DIR: str = "specs"
    CACHE_DIR: str = os.path.expanduser("~/.cache/product_refinement")
    CACHE_EXPIRY: int = 60 * 60 * 24 * 7  # 1 week in seconds
    
    @classmethod
    def from_args(cls, args):
        """Initialize config from command line args."""
        config = cls()
        config.MODEL_NAME = args.model
        config.LOG_LEVEL = args.log_level
        return config

class AIService:
    """Service for interacting with AI models."""
    
    def __init__(self, config: Config):
        """Initialize the AI service with configuration."""
        self.config = config
        self.llm = None  # Lazy initialization
        self._load_prompts()
        
        # Create cache directory if it doesn't exist
        os.makedirs(self.config.CACHE_DIR, exist_ok=True)
    
    def _load_prompts(self) -> None:
        """Load all prompts from files."""
        try:
            self.initial_prompt = self._load_prompt_file("initial.txt")
            self.refinement_prompt = self._load_prompt_file("refinement.txt")
            self.final_refinement_prompt = self._load_prompt_file("final_refinement.txt")
        except ValueError as e:
            logging.error(f"Error loading prompts: {str(e)}")
            sys.exit(1)
    
    def _load_prompt_file(self, prompt_file: str) -> str:
        """
        Load a prompt from a file in the prompts directory.
        
        Args:
            prompt_file (str): Name of the prompt file to load
            
        Returns:
            str: Content of the prompt file
            
        Raises:
            ValueError: If the prompt file cannot be loaded
        """
        prompt_path = os.path.join(self.config.PROMPT_DIR, prompt_file)
        try:
            with open(prompt_path, "r") as f:
                return f.read().strip()
        except IOError as e:
            raise ValueError(f"Failed to load prompt file {prompt_file}: {str(e)}")
    
    def get_cache_key(self, method_name: str, *args: Any, **kwargs: Any) -> str:
        """Generate a cache key based on method name and arguments."""
        # Create a string representation of the arguments
        args_str = str(args) + str(sorted(kwargs.items()))
        
        # Add model name to make cache key model-specific
        model_key = f"{self.config.MODEL_NAME}:{args_str}"
        
        # Create a hash of the arguments
        key = hashlib.md5(f"{method_name}:{model_key}".encode()).hexdigest()
        return key
    
    def get_cached_response(self, cache_key: str) -> Optional[str]:
        """Try to get a cached response."""
        cache_file = os.path.join(self.config.CACHE_DIR, cache_key)
        
        if os.path.exists(cache_file):
            # Check if cache has expired
            if time.time() - os.path.getmtime(cache_file) > self.config.CACHE_EXPIRY:
                logging.debug(f"Cache expired for key {cache_key}")
                return None
                
            try:
                with open(cache_file, 'rb') as f:
                    cached_data = pickle.load(f)
                    logging.debug(f"Cache hit for key {cache_key}")
                    return cached_data
            except Exception as e:
                logging.warning(f"Error reading cache: {e}")
                
        return None
    
    def save_to_cache(self, cache_key: str, data: Any) -> None:
        """Save response to cache."""
        cache_file = os.path.join(self.config.CACHE_DIR, cache_key)
        
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(data, f)
            logging.debug(f"Saved to cache: {cache_key}")
        except Exception as e:
            logging.warning(f"Error saving to cache: {e}")
    
    def cached_ai_call(method):
        """Decorator to cache AI calls."""
        @functools.wraps(method)
        def wrapper(self, *args, **kwargs):
            # Generate cache key
            cache_key = self.get_cache_key(method.__name__, *args, **kwargs)
            
            # Try to get cached response
            cached_response = self.get_cached_response(cache_key)
            if cached_response is not None:
                return cached_response
            
            # Call original method if no cache hit
            result = method(self, *args, **kwargs)
            
            # Save result to cache
            self.save_to_cache(cache_key, result)
            
            return result
        return wrapper
    
    def _get_model(self, max_retries: int = 3, retry_delay: float = 2.0):
        """Get the AI model with retry logic."""
        if self.llm is not None:
            return self.llm
            
        import importlib.util
        
        if importlib.util.find_spec("llm") is None:
            error_msg = "Error: The 'llm' package is not installed. Please install it with 'pip install llm'."
            logging.error(error_msg)
            print(error_msg)
            return None
            
        import llm
        
        for attempt in range(max_retries):
            try:
                self.llm = llm.get_model(self.config.MODEL_NAME)
                return self.llm
            except ConnectionError as e:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)  # Exponential backoff
                    logging.warning(f"Connection error: {e}. Retrying in {wait_time:.1f} seconds...")
                    time.sleep(wait_time)
                else:
                    logging.error(f"Failed to connect to AI model after {max_retries} attempts: {e}")
                    print(f"Error: Could not connect to the AI model. Please check your internet connection.")
                    return None
            except Exception as e:
                logging.error(f"Error getting AI model: {e}")
                print(f"Error: Could not load the AI model '{self.config.MODEL_NAME}'. Try using a different model.")
                return None
        
        return None

    def ask(self, prompt: str, model_name: Optional[str] = None, stream: bool = True, show_response: bool = True) -> str:
        """
        Calls the AI model using the llm library.

        Args:
            prompt (str): The input prompt to the AI model.
            model_name (str, optional): The name of the model to use. If None, uses the default from config.
            stream (bool): Whether to stream the response.
            show_response (bool): Whether to print the response when not streaming.

        Returns:
            str: The full AI-generated response.
        """
        # Set model name if provided, otherwise use default
        if model_name:
            current_model_name = self.config.MODEL_NAME
            self.config.MODEL_NAME = model_name
            self.llm = None  # Reset to force reload with new model
        
        try:
            # Get the model
            model = self._get_model()
            if not model:
                return "ERROR: Could not access the AI model."
            
            # Generate the response
            if stream:
                print(f"\n🤖 AI Response (Streaming with {self.config.MODEL_NAME})...")
                response_text = ""
                # Stream the response as it's generated
                for chunk in model.prompt(prompt):
                    print(chunk, end="", flush=True)
                    response_text += chunk
                print("\n")
                return response_text.strip()
            else:
                response = model.prompt(prompt)
                response_text = response.text()
                
                if show_response:
                    print(f"\n🤖 AI Response (Using {self.config.MODEL_NAME})...")
                    print(response_text)
                    print("\n")
                else:
                    print(f"\n🤖 Processing AI response (Using {self.config.MODEL_NAME})...")
                
                return response_text.strip()
                
        except Exception as e:
            logging.error(f"Error calling AI model: {str(e)}")
            return f"ERROR: Problem with the AI model response. {str(e)}"
        finally:
            # Restore original model name if it was changed
            if model_name:
                self.config.MODEL_NAME = current_model_name
                self.llm = None  # Reset to force reload with original model

    @cached_ai_call
    def generate_initial_spec(self, description: str) -> str:
        """
        Generate an initial product specification using AI.
        
        Args:
            description (str): Brief description of the product
            
        Returns:
            str: Initial product specification
        """
        prompt = self.initial_prompt + f"\n\nProduct description: {description}"
        response = self.ask(prompt, stream=False, show_response=False)
        
        if response.startswith("ERROR:"):
            print(f"\n⚠️ {response}")
            user_choice = ask_user("Would you like to try again with a different model? (yes/no)")
            if user_choice.lower().startswith('y'):
                model_name = ask_user("Please enter the model name to try:")
                print(f"\n🔄 Retrying with {model_name}...")
                response = self.ask(prompt, model_name=model_name, stream=False, show_response=False)
            else:
                print("\n⚠️ Continuing with empty specification. You may need to add more details manually.")
                return "Error occurred during generation. Please add specification details manually."
        
        return response
    
    @cached_ai_call
    def get_follow_up_questions(self, spec: str, answered_questions_text: str) -> List[Dict[str, str]]:
        """
        Get follow-up questions about the specification from the AI.
        
        Args:
            spec (str): The current specification
            answered_questions_text (str): Text describing previously answered questions
            
        Returns:
            List[Dict[str, str]]: List of questions with 'section' and 'question' keys
        """
        refinement_prompt = self.refinement_prompt.format(
            spec=spec,
            answered_questions=answered_questions_text
        )
        response = self.ask(refinement_prompt, stream=False, show_response=False)
        
        if response.startswith("ERROR:"):
            logging.error(f"Error in get_follow_up_questions: {response}")
            # Return an empty array to prevent further errors
            return []
        
        # Parse the JSON response
        try:
            questions = json.loads(response)
            if not isinstance(questions, list):
                logging.error("AI response is not a JSON array")
                return []
                
            # Validate each question has the expected fields
            valid_questions = []
            for question in questions:
                if isinstance(question, dict) and 'section' in question and 'question' in question:
                    valid_questions.append(question)
                else:
                    logging.warning(f"Skipping invalid question format: {question}")
                    
            return valid_questions
            
        except json.JSONDecodeError as e:
            logging.error(f"Failed to parse AI response as JSON: {e}")
            # Try to extract questions if JSON parsing failed
            return self._extract_questions_from_text(response)
    
    def _extract_questions_from_text(self, text: str) -> List[Dict[str, str]]:
        """
        Fallback method to extract questions from text if JSON parsing fails.
        
        Args:
            text (str): The text to extract questions from
            
        Returns:
            List[Dict[str, str]]: List of extracted questions
        """
        questions = []
        
        # Simple heuristic: look for lines that might contain questions
        lines = text.split('\n')
        current_section = "General"
        
        for line in lines:
            line = line.strip()
            
            # Try to identify section headers
            if line.endswith(':') and not line.startswith('"') and len(line) < 50:
                current_section = line.rstrip(':').strip()
                continue
            
            # Clean up JSON-like text
            if line.startswith('"question":'):
                line = line.replace('"question":', '').strip()
                if line.startswith('"') and line.endswith('"'):
                    line = line[1:-1]  # Remove quotes
            
            # Look for question marks
            if '?' in line and len(line) > 10:
                # Clean up any remaining JSON artifacts
                line = line.strip('",')
                questions.append({
                    'section': current_section,
                    'question': line
                })
                
        logging.info(f"Extracted {len(questions)} questions using fallback method")
        return questions[:1]  # Return at most one question to avoid flooding the user
    
    @cached_ai_call
    def finalize_spec(self, spec: str) -> str:
        """
        Generate the final well-structured product specification using AI.
        
        Args:
            spec (str): The refined product specification
            
        Returns:
            str: The finalized product specification
        """
        print("\n📌 Finalizing the specification with AI...")
        final_prompt = self.final_refinement_prompt.format(spec=spec)
        response = self.ask(final_prompt, stream=False, show_response=False)
        
        if response.startswith("ERROR:"):
            print(f"\n⚠️ {response}")
            print("\n⚠️ Unable to finalize the specification. Returning the unfinalized version.")
            return spec
        
        return response
    
    @cached_ai_call
    def suggest_project_name(self, spec: str) -> str:
        """
        Get an AI suggestion for a project name based on the specification.
        
        Args:
            spec (str): The product specification
            
        Returns:
            str: The suggested project name
        """
        prompt = """
        Based on this product specification, suggest a concise, memorable project name.
        Return ONLY the suggested name, nothing else.
        
        Specification:
        {spec}
        """.format(spec=spec)
        
        response = self.ask(prompt, stream=False, show_response=True)
        
        if response.startswith("ERROR:"):
            print(f"\n⚠️ {response}")
            return "untitled_project"
        
        return response.strip()

class SpecificationManager:
    """Manages specification file operations."""
    
    def __init__(self, config: Config):
        """Initialize the specification manager with configuration."""
        self.config = config
        os.makedirs(self.config.SPECS_DIR, exist_ok=True)
    
    def get_project_dir(self, product_name: str) -> str:
        """
        Get the directory path for a project, creating it if it doesn't exist.
        
        Args:
            product_name (str): Name of the product/project
            
        Returns:
            str: Path to the project directory
        """
        # Convert product name to a valid directory name
        dir_name = "".join(c.lower() for c in product_name if c.isalnum())
        project_dir = os.path.join(self.config.SPECS_DIR, dir_name)
        os.makedirs(project_dir, exist_ok=True)
        return project_dir
    
    def get_next_version(self, project_dir: str) -> int:
        """
        Get the next version number for a project.
        
        Args:
            project_dir (str): Path to the project directory
            
        Returns:
            int: Next version number
        """
        versions = [0]  # Start with 0 if no versions exist
        for filename in os.listdir(project_dir):
            if filename.endswith(".json"):
                try:
                    version = int(filename.split("_v")[1].split(".")[0])
                    versions.append(version)
                except (IndexError, ValueError):
                    continue
        return max(versions) + 1
    
    def format_timestamp(self, timestamp_str: str) -> str:
        """
        Format a timestamp string into a human-readable format.
        
        Args:
            timestamp_str (str): Timestamp in format "YYYYMMDD_HHMMSS"
            
        Returns:
            str: Formatted date and time (e.g., "2024-03-20 15:30:45")
        """
        try:
            dt = datetime.strptime(timestamp_str, "%Y%m%d_%H%M%S")
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        except ValueError:
            return timestamp_str
    
    def save_specification(self, spec: str, product_name: Optional[str] = None) -> Tuple[str, int]:
        """
        Save the specification to files in both Markdown and JSON formats.
        
        Args:
            spec (str): The product specification text
            product_name (str, optional): Name of the product
            
        Returns:
            Tuple[str, int]: Project name and version number
        """
        if not product_name:
            try:
                product_name = spec.split("Product Name:")[1].split("\n")[0].strip()
            except IndexError:
                product_name = "unnamed_project"
        
        project_dir = self.get_project_dir(product_name)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        version = self.get_next_version(project_dir)
        
        # Create filenames with version numbers
        base_filename = f"spec_v{version}"
        
        # Add timestamp to the specification content
        formatted_timestamp = self.format_timestamp(timestamp)
        spec_with_metadata = f"Version: {version}\nLast Updated: {formatted_timestamp}\n\n{spec}"
        
        # Save Markdown version
        md_filename = os.path.join(project_dir, f"{base_filename}.md")
        with open(md_filename, "w") as f:
            f.write(spec_with_metadata)
        
        # Convert to JSON structure
        spec_dict: SpecificationData = {
            "version": version,
            "timestamp": timestamp,
            "formatted_timestamp": formatted_timestamp,
            "product_name": product_name,
            "specification": spec_with_metadata
        }
        
        # Save JSON version
        json_filename = os.path.join(project_dir, f"{base_filename}.json")
        with open(json_filename, "w") as f:
            json.dump(spec_dict, f, indent=2, ensure_ascii=False)
        
        print(f"\n📁 Specification v{version} saved to project '{product_name}':")
        print(f"   - Markdown: {md_filename}")
        print(f"   - JSON: {json_filename}")
        print(f"   - Last Updated: {formatted_timestamp}")
        
        return product_name, version
    
    def list_specifications(self) -> Dict[str, List[SpecificationVersion]]:
        """
        List all available specifications grouped by project.
        
        Returns:
            Dict mapping project directories to lists of specification versions.
        """
        if not os.path.exists(self.config.SPECS_DIR):
            return {}
        
        projects: Dict[str, List[SpecificationVersion]] = {}
        for project_dir in os.listdir(self.config.SPECS_DIR):
            project_path = os.path.join(self.config.SPECS_DIR, project_dir)
            if not os.path.isdir(project_path):
                continue
            
            specs: List[SpecificationVersion] = []
            for filename in os.listdir(project_path):
                if filename.endswith(".json"):
                    filepath = os.path.join(project_path, filename)
                    try:
                        with open(filepath, "r") as f:
                            data = json.load(f)
                            timestamp = data.get("timestamp", "Unknown")
                            spec_version: SpecificationVersion = {
                                "filename": filename,
                                "version": data.get("version", 0),
                                "timestamp": timestamp,
                                "formatted_timestamp": data.get(
                                    "formatted_timestamp", 
                                    self.format_timestamp(timestamp)
                                ),
                                "product_name": data.get("product_name", project_dir)
                            }
                            specs.append(spec_version)
                    except (json.JSONDecodeError, IOError):
                        continue
            
            if specs:
                specs.sort(key=lambda x: x["version"], reverse=True)
                projects[project_dir] = specs
        
        return projects
    
    def load_specification(self, filename: str) -> Tuple[Optional[str], Optional[str]]:
        """
        Load a specification from a JSON file.
        
        Args:
            filename (str): Name of the JSON file to load
            
        Returns:
            tuple: (specification_text, project_name) or (None, None) if loading fails
        """
        filepath = os.path.join(self.config.SPECS_DIR, filename)
        try:
            with open(filepath, "r") as f:
                data: SpecificationData = json.load(f)
                return data.get("specification", ""), data.get("product_name")
        except (json.JSONDecodeError, IOError) as e:
            print(f"Error loading specification: {e}")
            return None, None

# Create a console for rich output
console = Console() if RICH_AVAILABLE else None

def format_spec_as_markdown(spec: str) -> Any:
    """Format specification as markdown if rich is available."""
    if RICH_AVAILABLE:
        return Markdown(spec)
    return spec

def display_banner(title: str) -> None:
    """Display a banner with the title."""
    if RICH_AVAILABLE:
        console.print(Panel(f"[bold]{title}[/bold]", border_style="blue"))
    else:
        print("\n" + "=" * 80)
        print(f" {title} ".center(80))
        print("=" * 80)

def ask_user(prompt: str) -> str:
    """Ask user for input with enhanced UI if available."""
    if RICH_AVAILABLE:
        return Prompt.ask(f"[bold cyan]{prompt}[/bold cyan]")
    else:
        return input(f"{prompt} ")

def display_success(message: str) -> None:
    """Display a success message."""
    if RICH_AVAILABLE:
        console.print(f"[bold green]✓[/bold green] {message}")
    else:
        print(f"✓ {message}")

def display_error(message: str) -> None:
    """Display an error message."""
    if RICH_AVAILABLE:
        console.print(f"[bold red]✗[/bold red] {message}")
    else:
        print(f"✗ {message}")

def display_warning(message: str) -> None:
    """Display a warning message."""
    if RICH_AVAILABLE:
        console.print(f"[bold yellow]⚠[/bold yellow] {message}")
    else:
        print(f"⚠ {message}")

def display_info(message: str) -> None:
    """Display an informational message."""
    if RICH_AVAILABLE:
        console.print(f"[bold blue]ℹ[/bold blue] {message}")
    else:
        print(f"ℹ {message}")

def validate_input(prompt: str, validators: List[Callable[[str], Any]] = None) -> Any:
    """Ask for user input and validate it against a list of validators."""
    while True:
        try:
            value = ask_user(prompt)
            
            if validators:
                for validator in validators:
                    value = validator(value)
            
            return value
        except ValidationError as e:
            display_error(str(e))
            # Continue the loop to ask again

def choose_specification() -> Tuple[bool, Optional[Tuple[str, str]]]:
    """Let the user choose an existing specification or create a new one."""
    from tempfile import gettempdir
    config = Config()
    spec_manager = SpecificationManager(config)
    all_projects = spec_manager.list_specifications()
    
    if not all_projects:
        if RICH_AVAILABLE:
            console.print("[yellow]No existing specifications found.[/yellow]")
        else:
            print("No existing specifications found.")
        return True, None

    if RICH_AVAILABLE:
        # Create a table for specifications
        table = Table(title="Available Specifications")
        table.add_column("ID", style="dim")
        table.add_column("Product Name", style="green")
        table.add_column("Version", style="blue")
        table.add_column("Created", style="magenta")
        
        # Flatten the projects and specs
        flat_specs = []
        for project_dir, specs in all_projects.items():
            for spec in specs:
                flat_specs.append((project_dir, spec))
        
        # Add rows to the table
        for i, (project_dir, spec) in enumerate(flat_specs):
            table.add_row(
                str(i+1),
                spec['product_name'],
                f"v{spec['version']}",
                spec['formatted_timestamp']
            )
        
        console.print(table)
        console.print("Enter the ID of the specification to edit, or 'new' to create a new one:")
        choice = Prompt.ask("", default="new")
    else:
        print("\nAvailable specifications:")
        flat_specs = []
        for i, (project_dir, specs) in enumerate(all_projects.items()):
            for j, spec in enumerate(specs):
                idx = len(flat_specs) + 1
                flat_specs.append((project_dir, spec))
                print(f"{idx}. {spec['product_name']} (v{spec['version']}, {spec['formatted_timestamp']})")
        
        print("\nEnter the number of the specification to edit, or 'new' to create a new one:")
        choice = input("> ").strip()
    
    if choice.lower() == 'new':
        return True, None
    
    try:
        idx = int(choice) - 1
        if 0 <= idx < len(flat_specs):
            project_dir, spec_info = flat_specs[idx]
            filename = spec_info['filename']
            spec, project_name = spec_manager.load_specification(os.path.join(project_dir, filename))
            if spec:
                display_success(f"Loaded {project_name} v{spec_info['version']}")
                return False, (spec, project_name)
    except (ValueError, IndexError):
        display_error("Invalid selection")
    
    display_warning("Creating a new specification instead")
    return True, None

def refine_spec(spec_text: str) -> str:
    """Refine the specification by iteratively asking follow-up questions."""
    ai_service = AIService(Config())
    answered_questions: List[AnsweredQuestion] = []
    current_spec = spec_text
    
    display_banner("Refining Your Specification")
    display_info("I'll ask you questions to help refine your specification.")
    display_info("Type 'done' when you're satisfied, or 'skip' to skip a question.")
    
    question_count = 0
    max_questions = 10
    
    while question_count < max_questions:
        # Show progress
        if RICH_AVAILABLE:
            console.print(f"\n[bold]Refinement Progress: {question_count}/{max_questions}[/bold]")
        else:
            print(f"\nRefinement Progress: {question_count}/{max_questions}")
        
        # Get follow-up questions
        with Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]Generating questions...[/bold blue]"),
            transient=True
        ) if RICH_AVAILABLE else DummyProgress() as progress:
            progress.start()
            questions = ai_service.get_follow_up_questions(current_spec, format_answered_questions(answered_questions))
            progress.stop()
        
        if not questions:
            display_warning("No more questions to ask. Finalizing specification.")
            break
        
        question = questions[0]  # Get the first question
        question_section = question.get('section', 'General')
        question_text = question.get('question', '')
        
        # Display the question in a clean format
        if RICH_AVAILABLE:
            console.print(f"\n[bold]📌 {question_section}[/bold]")
            console.print(f"[cyan]{question_text}[/cyan]")
        else:
            print(f"\n📌 {question_section}:")
            print(f"{question_text}")
        
        # Process user response and update spec
        updated_spec, should_exit, should_skip = process_user_response(
            question_section, question_text, answered_questions, current_spec
        )
        
        if should_exit:
            break
            
        if should_skip:
            continue
            
        current_spec = updated_spec
        question_count += 1
        display_success("Specification updated!")
    
    return current_spec

# Dummy progress class for when rich is not available
class DummyProgress:
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        pass
    
    def start(self):
        print("Processing...")
    
    def stop(self):
        pass

def setup_argument_parser() -> argparse.ArgumentParser:
    """Set up command line argument parsing with subcommands."""
    
    # Main parser
    parser = argparse.ArgumentParser(
        description='AI-Powered Product Specification Generator',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Create a new specification
  python spec.py create
  
  # List existing specifications
  python spec.py list
  
  # Edit an existing specification
  python spec.py edit
        """
    )
    
    # Global options
    parser.add_argument(
        '--log-level', 
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        help='Set the logging level'
    )
    parser.add_argument(
        '--model',
        default='gemini-2.0-flash',
        help='Select the AI model to use (any model supported by llm)'
    )
    
    # Subparsers for commands
    subparsers = parser.add_subparsers(dest='command', help='Command to execute')
    
    # Create a new specification
    create_parser = subparsers.add_parser('create', help='Create a new specification')
    create_parser.add_argument('--description', help='Initial product description')
    
    # List existing specifications
    list_parser = subparsers.add_parser('list', help='List existing specifications')
    list_parser.add_argument('--project', help='Filter by project name')
    
    # Edit an existing specification
    edit_parser = subparsers.add_parser('edit', help='Edit an existing specification')
    edit_parser.add_argument('--project', help='Project name to edit')
    edit_parser.add_argument('--version', type=int, help='Version number to edit (defaults to latest)')
    
    return parser

def format_answered_questions(answered_questions: List[AnsweredQuestion]) -> str:
    """
    Format the list of answered questions for the AI prompt.
    
    Args:
        answered_questions (List[AnsweredQuestion]): List of answered questions
        
    Returns:
        str: Formatted text of answered questions
    """
    if not answered_questions:
        return "No questions answered yet."
        
    return "\n".join([
        f"Section: {q['section']}\nQuestion: {q['question']}\nAnswer: {q['answer']}"
        for q in answered_questions
    ])

def process_user_response(
    section: str, 
    question: str, 
    answered_questions: List[AnsweredQuestion],
    spec: str
) -> Tuple[str, bool, bool]:
    """
    Process the user's response to a follow-up question.
    
    Args:
        section (str): The section being refined
        question (str): The question being asked
        answered_questions (List[AnsweredQuestion]): List to store answered questions
        spec (str): Current specification text
        
    Returns:
        Tuple containing:
        - Updated specification text
        - Boolean indicating whether to exit refinement
        - Boolean indicating whether to skip this question
    """
    user_input = ask_user("\nProvide your answer (or type 'skip' to move on, 'done' to finish refinements).")

    if user_input.lower() == "done":
        print("\n✅ Exiting refinements early.")
        return spec, True, False
        
    if user_input.lower() == "skip":
        print("⏭️ Skipping this question.")
        return spec, False, True
        
    # Store the answered question and response
    answered_question: AnsweredQuestion = {
        "section": section,
        "question": question,
        "answer": user_input
    }
    answered_questions.append(answered_question)

    # Update the spec with user's response
    updated_spec = spec + f"\n\n**{section}:** {question}\n{user_input}"
    
    return updated_spec, False, False

if __name__ == "__main__":
    # Set up and parse arguments
    parser = setup_argument_parser()
    args = parser.parse_args()
    
    # Create configuration
    config = Config.from_args(args)
    
    # Configure logging
    logging.basicConfig(level=getattr(logging, config.LOG_LEVEL))
    logging.getLogger("httpx").setLevel(logging.WARNING)
    
    # Load environment variables
    load_dotenv()
    
    display_banner("AI-Powered Product Specification Generator")
    
    # Create services
    ai_service = AIService(config)
    spec_manager = SpecificationManager(config)
    
    # Handle different commands
    if args.command == 'list':
        # List specifications
        with Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]Loading specifications...[/bold blue]"),
            transient=True
        ) if RICH_AVAILABLE else DummyProgress() as progress:
            progress.start()
            projects = spec_manager.list_specifications()
            progress.stop()
            
        if not projects:
            display_warning("No specifications found.")
            sys.exit(0)
            
        if RICH_AVAILABLE:
            # Create a table for specifications
            table = Table(title="Product Specifications")
            table.add_column("Product Name", style="green")
            table.add_column("Latest Version", style="blue")
            table.add_column("Last Updated", style="magenta")
            table.add_column("Total Versions", style="cyan")
            
            for project_dir, specs in sorted(projects.items()):
                latest_spec = specs[0]  # First spec is the newest
                table.add_row(
                    latest_spec['product_name'],
                    f"v{latest_spec['version']}",
                    latest_spec['formatted_timestamp'],
                    str(len(specs))
                )
                
                # If a specific project was requested, show all versions
                if args.project and args.project.lower() in project_dir.lower():
                    for spec in specs:
                        table.add_row(
                            f"└─ v{spec['version']}",
                            "",
                            spec['formatted_timestamp'],
                            ""
                        )
            
            console.print(table)
        else:
            print("\nAvailable projects:")
            for project_dir, specs in sorted(projects.items()):
                latest_spec = specs[0]  # First spec is the newest
                print(f"  {latest_spec['product_name']} (latest: v{latest_spec['version']}, updated: {latest_spec['formatted_timestamp']})")
                # If a specific project was requested, show all versions
                if args.project and args.project.lower() in project_dir.lower():
                    print("  Versions:")
                    for spec in specs:
                        print(f"    v{spec['version']} (created: {spec['formatted_timestamp']})")
        
    elif args.command == 'create' or args.command == 'edit' or not args.command:
        is_new: bool
        loaded_spec: Optional[Tuple[str, str]]
        
        # For create command, always create new
        if args.command == 'create':
            is_new = True
            loaded_spec = None
        # For edit command, try to load based on args
        elif args.command == 'edit' and args.project:
            with Progress(
                SpinnerColumn(),
                TextColumn("[bold blue]Searching for project...[/bold blue]"),
                transient=True
            ) if RICH_AVAILABLE else DummyProgress() as progress:
                progress.start()
                projects = spec_manager.list_specifications()
                progress.stop()
                
            found = False
            
            for project_dir, specs in projects.items():
                # Check if project name matches
                if args.project.lower() in specs[0]['product_name'].lower() or args.project.lower() in project_dir.lower():
                    found = True
                    # Find the right version
                    spec_index = 0  # Default to latest
                    if args.version is not None:
                        for i, spec in enumerate(specs):
                            if spec['version'] == args.version:
                                spec_index = i
                                break
                        else:
                            display_warning(f"Version {args.version} not found, using latest.")
                    
                    with Progress(
                        SpinnerColumn(),
                        TextColumn("[bold blue]Loading specification...[/bold blue]"),
                        transient=True
                    ) if RICH_AVAILABLE else DummyProgress() as progress:
                        progress.start()
                        filename = specs[spec_index]['filename']
                        spec, project_name = spec_manager.load_specification(os.path.join(project_dir, filename))
                        progress.stop()
                        
                    if spec:
                        is_new = False
                        loaded_spec = (spec, project_name)
                        display_success(f"Loaded {project_name} v{specs[spec_index]['version']}")
                        break
            
            if not found:
                display_error(f"Project '{args.project}' not found.")
                is_new, loaded_spec = choose_specification()
        else:
            # Interactive mode for other cases
            is_new, loaded_spec = choose_specification()
        
        project_name: Optional[str] = None
        
        if is_new:
            display_banner("Creating New Specification")
            # If description was provided via CLI, use it
            if args.command == 'create' and args.description:
                product_description = args.description
            else:
                # Validate product description - must be at least 20 chars
                product_description = validate_input(
                    "Describe your product in a few sentences:",
                    [
                        Validator.not_empty,
                        lambda x: Validator.min_length(x, 20, "Please provide a more detailed description (at least 20 characters)")
                    ]
                )
                
            if RICH_AVAILABLE:
                with Progress(
                    "[progress.description]{task.description}",
                    SpinnerColumn(),
                    BarColumn(),
                    TextColumn("[bold blue]{task.fields[status]}")
                ) as progress:
                    task = progress.add_task(
                        "[cyan]Generating specification...",
                        status="Working on it..."
                    )
                    spec = ai_service.generate_initial_spec(product_description)
                    progress.update(task, completed=True, status="Done!")
            else:
                print("\n🚀 Generating initial specification draft...")
                spec = ai_service.generate_initial_spec(product_description)
                
            if RICH_AVAILABLE:
                console.print("\n📄 [bold]Initial Specification Draft:[/bold]")
                console.print(format_spec_as_markdown(spec))
            else:
                print("\n📄 Initial Specification Draft:\n")
                print(spec)
        else:
            spec, project_name = loaded_spec  # Now returns both spec and name
            
            if RICH_AVAILABLE:
                console.print("\n📄 [bold]Loaded Specification:[/bold]")
                console.print(format_spec_as_markdown(spec))
            else:
                print("\n📄 Loaded Specification:\n")
                print(spec)
        
        updated_spec = refine_spec(spec)
        
        if RICH_AVAILABLE:
            with Progress(
                "[progress.description]{task.description}",
                SpinnerColumn(),
                BarColumn(),
                TextColumn("[bold blue]{task.fields[status]}")
            ) as progress:
                task = progress.add_task(
                    "[cyan]Finalizing specification...",
                    status="Working on it..."
                )
                final_spec = ai_service.finalize_spec(updated_spec)
                progress.update(task, completed=True, status="Done!")
        else:
            print("\n📌 Finalizing the specification...")
            final_spec = ai_service.finalize_spec(updated_spec)
            
        if RICH_AVAILABLE:
            console.print("\n✅ [bold green]FINAL PRODUCT SPECIFICATION:[/bold green]")
            console.print(format_spec_as_markdown(final_spec))
        else:
            print("\n✅ FINAL PRODUCT SPECIFICATION:\n")
            print(final_spec)
        
        # Only suggest name for new specs
        if is_new:
            with Progress(
                SpinnerColumn(),
                TextColumn("[bold blue]Suggesting project name...[/bold blue]"),
                transient=True
            ) if RICH_AVAILABLE else DummyProgress() as progress:
                progress.start()
                suggested_name = ai_service.suggest_project_name(final_spec)
                progress.stop()
                
            if RICH_AVAILABLE:
                console.print(f"\n🤖 [bold]Suggested project name:[/bold] [green]{suggested_name}[/green]")
            else:
                print(f"\n🤖 Suggested project name: {suggested_name}")
            
            # Validate yes/no response
            use_suggested = validate_input(
                "Would you like to use this name? (yes/no)",
                [Validator.is_yes_no]
            )
            
            if use_suggested:
                project_name = suggested_name
                display_success(f"Using name: {project_name}")
            else:
                # Validate custom project name
                project_name = validate_input(
                    "Please enter your preferred project name:",
                    [
                        Validator.not_empty,
                        Validator.is_valid_filename,
                        lambda x: Validator.max_length(x, 50, "Project name too long (max 50 characters)")
                    ]
                )
                display_success(f"Using name: {project_name}")
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]Saving specification...[/bold blue]"),
            transient=True
        ) if RICH_AVAILABLE else DummyProgress() as progress:
            progress.start()
            saved_path = spec_manager.save_specification(final_spec, project_name)
            progress.stop()
            
        display_success(f"Specification saved to {saved_path}")
        
    else:
        parser.print_help()
